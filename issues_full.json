[{"body":"Description: Finalize the project for public consumption. This involves writing comprehensive user documentation, polishing the codebase to meet professional standards, and configuring the packaging metadata for the Python Package Index (PyPI). This is the \"Productization\" phase of Pathos AI.\n\nObjectives:\n\n    Create a high-quality README.md that serves as the \"front door\" of the project.\n\n    Generate a full API reference from the Google-style docstrings.\n\n    Ensure the package is \"installable\" via pip with correct versioning.\n\n    Finalize the CONCEPTS.md \"Project Bible\" as an educational resource for users.\n\nTasks:\n\n    [ ] Write the README.md:\n\n        Include a clear \"Quick Start\" guide showing A* and Genetic Algorithm examples.\n\n        Add the Benchmark results from Issue #11 to show performance.\n\n        Include clear installation instructions and a description of the \"Hybrid-Native\" philosophy.\n\n    [ ] Docstring Audit:\n\n        Review all files (core.py, searching/, etc.) to ensure every public class and function has a Google-style docstring with Args, Returns, and Raises.\n\n    [ ] Finalize pyproject.toml:\n\n        Add project metadata: Description, Author, License (MIT/Apache), and Keywords.\n\n        Configure \"Entry Points\" if we want any command-line tools (like a pathos-bench command).\n\n    [ ] Create Examples Gallery:\n\n        Create an examples/ folder with well-commented scripts:\n\n            maze_solver.py (A*)\n\n            n_queens.py (CSP)\n\n            tictactoe_bot.py (Minimax/MCTS)\n\n            function_optimization.py (PSO/DE)\n\n    [ ] Build Check:\n\n        Run python -m build to ensure the source distribution and wheel are generated without errors.\n\nAcceptance Criteria:\n\n    [ ] A user can run pip install . and then import pathos in a separate script without errors.\n\n    [ ] The README.md contains at least three code snippets that run \"out of the box.\"\n\n    [ ] All tests pass with pytest, and mypy reports zero type errors.\n\n    [ ] The project structure is clean, with no temporary or junk files (__pycache__, .pytest_cache) tracked in Git.\n\nDependencies:\n\nIssues #1 through #11 (The entire library must be functional).","labels":[],"number":12,"state":"OPEN","title":"Issue #12 – Documentation & PyPI Release Preparation"},{"body":"Description: Conduct a comprehensive performance audit of the Pathos AI library. This involves using profiling tools to identify bottlenecks in the Node.expand logic and the heavy-lifting optimization loops. We will establish \"Performance Baselines\" to ensure future updates don't make the library slower.\n\nObjectives:\n\n    Identify \"Hot Spots\" in the code (functions that take the most time).\n\n    Compare the memory overhead of the Node class with and without __slots__.\n\n    Verify that Numpy-based algorithms (GA, PSO) scale better than pure Python loops as problem size increases.\n\n    Create a benchmarks/ suite to track execution time across different Python versions.\n\nTasks:\n\n    [ ] Implement Profiling Scripts:\n\n        Use cProfile and line_profiler to analyze a deep A* search and a large Genetic Algorithm run.\n\n    [ ] Memory Auditing:\n\n        Use sys.getsizeof or tracemalloc to measure the memory footprint of 1,000,000 Node objects.\n\n    [ ] The \"Numpy vs. Loop\" Showdown:\n\n        Create a test that solves the same problem using a pure Python loop vs. a vectorized Numpy operation to document the speedup factor (e.g., 10x, 50x).\n\n    [ ] Optimization Refactoring:\n\n        Based on profiling results, refactor any slow sections in core.py (e.g., optimizing the f_score property).\n\n    [ ] Automated Benchmarking:\n\n        Create scripts/run_benchmarks.py that outputs a markdown table of results for the README.\n\nAcceptance Criteria:\n\n    [ ] A profiling report is generated showing that Node.expand is efficient.\n\n    [ ] Memory usage for 1 million nodes is documented (should be significantly lower thanks to __slots__).\n\n    [ ] A performance regression test is added to the CI/CD pipeline (optional but recommended).\n\nDependencies:\n\nIssue #2 through Issue #10 (Requires all modules to be complete).","labels":[],"number":11,"state":"OPEN","title":"Issue #11 – Profiling, Benchmarking & Performance Tuning"},{"body":"Description: Implement Differential Evolution (DE) in optimization/diff_evo.py and Simulated Annealing (SA) in optimization/annealing.py. These two algorithms provide the highest level of robustness in our toolkit. DE is a population-based vector optimizer, while SA is a single-state trajectory optimizer that uses \"probabilistic jumping\" to find global optima.\n\nObjectives:\n\n    Implement the Differential Mutation strategy (u=a+F(b−c)) for DE.\n\n    Implement the Metropolis Criterion for SA: accepting worse solutions based on temperature.\n\n    Create a Cooling Schedule system (Linear and Geometric) for Simulated Annealing.\n\n    Finalize the optimization module with a benchmark script comparing all four methods (GA, PSO, DE, SA).\n\nTasks:\n\n    [ ] Implement optimization/diff_evo.py:\n\n        Logic: For each individual, pick three random others (a,b,c). Create a mutant vector by adding the weighted difference of b and c to a.\n\n        Implement Binomial Crossover: Decide gene-by-gene whether to take from the mutant or the original.\n\n    [ ] Implement optimization/annealing.py:\n\n        State Management: Unlike the others, SA only tracks one current state.\n\n        Probability Logic: P=e(new_fit−old_fit)/T. If a random number is less than P, accept the move even if it's worse.\n\n    [ ] Benchmark Suite:\n\n        Create scripts/benchmark_optimization.py.\n\n        Run all optimizers on the Rastrigin Function (a very \"bumpy\" function with many traps).\n\n    [ ] Unit Testing:\n\n        Ensure SA can solve a discrete problem (like a small Traveling Salesman Problem).\n\n        Ensure DE can solve high-dimensional continuous problems.\n\nAcceptance Criteria:\n\n    [ ] DE successfully minimizes a 10-dimensional mathematical function.\n\n    [ ] SA demonstrates \"hill-climbing\" and \"valley-jumping\" behavior by accepting worse moves at high temperatures.\n\n    [ ] The benchmark script produces a clear table or plot comparing the convergence speed of each algorithm.\n\nDependencies:\n\nIssue #8 (Optimization Infrastructure)","labels":[],"number":10,"state":"OPEN","title":"Issue #10 – Differential Evolution & Simulated Annealing"},{"body":"Description: Implement Particle Swarm Optimization (PSO) in optimization/pso.py. This algorithm uses Numpy to simulate a \"swarm\" of particles where each particle (potential solution) moves based on its own experience and the experience of its neighbors. This is highly effective for continuous mathematical functions.\n\nObjectives:\n\n    Implement the particle update physics: Velocity and Position.\n\n    Use Numpy vectorization to update the entire swarm simultaneously.\n\n    Implement the concepts of Personal Best (pbest) and Global Best (gbest).\n\n    Support inertia and acceleration coefficients to tune the swarm's behavior.\n\nTasks:\n\n    [ ] Implement optimization/pso.py:\n\n        optimize(fitness_func, swarm_size, dimensions, steps): The main loop.\n\n        Initialize X (positions) and V (velocities) as Numpy arrays.\n\n    [ ] The Velocity Update Formula:\n\n        Implement: Vt+1​=wVt​+c1​r1​(pbest−Xt​)+c2​r2​(gbest−Xt​)\n\n        w (inertia), c1​ (cognitive/personal), c2​ (social/global).\n\n    [ ] Boundary Handling:\n\n        Ensure particles stay within the defined search space (clipping).\n\n    [ ] Unit Testing:\n\n        Test the optimizer against a known function like the Sphere Function (where the goal is to reach 0 at coordinates [0,0]).\n\nAcceptance Criteria:\n\n    [ ] The swarm successfully converges on a target point in a multi-dimensional space.\n\n    [ ] The implementation uses zero Python loops for the velocity and position updates (100% Numpy).\n\n    [ ] The user can pass custom w,c1​,c2​ parameters.\n\nDependencies:\n\nIssue #1 (Project Initialization)\n\nIssue #8 (Optimization/Numpy Infrastructure)","labels":[],"number":9,"state":"OPEN","title":"Issue #9 – Particle Swarm Optimization (PSO)"},{"body":"Description: Implement the Genetic Algorithm (GA) in optimization/genetic.py. This module introduces the \"Heavy Weaponry\" of the library: Numpy. By representing our population as a 2D matrix, we can evaluate fitness, perform crossovers, and apply mutations to hundreds of individuals at once using vectorized operations.\n\nObjectives:\n\n    Establish the pattern for Soft Imports of Numpy so the library remains lightweight for users who only need core search features.\n\n    Implement a Vectorized Fitness Evaluation system.\n\n    Implement the three pillars of evolution: Tournament Selection, Crossover (Recombination), and Mutation.\n\n    Support both binary strings and real-valued (floating point) genotypes.\n\nTasks:\n\n    [ ] Implement Soft Numpy Imports:\n\n        In optimization/__init__.py, use try/except ImportError to handle environments without Numpy, providing clear warnings.\n\n    [ ] Develop optimization/genetic.py:\n\n        evolve(fitness_func, pop_size, genome_len, generations): The main loop.\n\n        Selection: Implement \"Tournament Selection\"—randomly pick k individuals and select the best.\n\n        Crossover: Implement \"Single-Point Crossover\" using Numpy slicing for speed.\n\n        Mutation: Use np.random.rand() to flip bits or add Gaussian noise to genes.\n\n    [ ] Vectorization Logic:\n\n        Ensure the fitness_func can receive the entire population matrix and return a 1D array of scores in one call.\n\n    [ ] Unit Testing:\n\n        Create tests/test_optimization.py.\n\n        Use GA to solve a simple optimization problem, such as maximizing the number of \"1s\" in a bitstring (OneMax problem).\n\nAcceptance Criteria:\n\n    [ ] The algorithm can evolve a population of 1,000 individuals for 100 generations in under a few seconds.\n\n    [ ] The crossover and mutation operations are implemented using Numpy broadcasting (no explicit Python for loops where possible).\n\n    [ ] The code includes a NumpyStateMixin in core.py (as per the spec) to handle hashing of Numpy-based states.\n\nDependencies:\n\nIssue #1 (Project Initialization)\n\nIssue #2 (Core Architecture)","labels":[],"number":8,"state":"OPEN","title":"Issue #8 – Optimization Infrastructure & Genetic Algorithms (GA)"},{"body":"Description: Implement the Monte Carlo Tree Search (MCTS) algorithm in adversarial/mcts.py. Unlike Minimax, which uses a fixed evaluation function, MCTS estimates the value of a move by running random \"rollouts\" (simulations) to the end of the game. It uses the UCT (Upper Confidence Bound for Trees) formula to balance exploring new moves vs. exploiting moves that have already shown success.\n\nObjectives:\n\n    Implement the 4-step MCTS cycle: Selection, Expansion, Simulation, and Backpropagation.\n\n    Implement the UCT formula to guide the search.\n\n    Support an \"Anytime\" search: The algorithm should be able to stop at any time and return the best move found so far based on visit counts.\n\n    Compare MCTS performance against Minimax for a 3x3 grid (Tic-Tac-Toe) and potentially a larger 4x4 grid.\n\nTasks:\n\n    [ ] Implement adversarial/mcts.py:\n\n        MCTSNode: A specialized node class that tracks visit_count and total_reward.\n\n        select(node): Use the UCT formula to navigate to a leaf.\n\n        expand(node): Add a new child for an untried action.\n\n        simulate(state): Play a random game until a terminal state is reached.\n\n        backpropagate(node, result): Update statistics from the leaf up to the root.\n\n    [ ] Implement the UCT Formula:\n\n        UCT=ni​wi​​+Cni​lnNi​​​\n\n        (wi​ = wins, ni​ = node visits, Ni​ = parent visits, C = exploration constant).\n\n    [ ] Testing & Benchmarking:\n\n        Verify that MCTS converges to the same \"best move\" as Minimax given enough simulations.\n\n        Document the memory usage vs. Minimax in CONCEPTS.md.\n\nAcceptance Criteria:\n\n    [ ] The MCTS agent can play a legal and competitive game of Tic-Tac-Toe.\n\n    [ ] The simulate function correctly reaches terminal states and returns the game outcome.\n\n    [ ] The \"Anytime\" property is verified (the AI returns a move even if given only 100 simulations).\n\nDependencies:\n\nIssue #2 (Core Architecture)\n\nIssue #6 (AdversarialGame Protocol)","labels":[],"number":7,"state":"OPEN","title":"Issue #7 – Monte Carlo Tree Search (MCTS)"},{"body":"Description: Implement the Minimax algorithm with Alpha-Beta Pruning in adversarial/minimax.py. This algorithm is the standard for two-player, zero-sum games (like Tic-Tac-Toe or Chess). It works by simulating the entire game tree, assuming both players play perfectly: you (MAX) try to maximize your score, while the opponent (MIN) tries to minimize it.\n\nObjectives:\n\n    Implement a recursive Minimax function.\n\n    Integrate Alpha-Beta Pruning to \"cut off\" branches of the search tree that cannot possibly influence the final decision, significantly improving performance.\n\n    Support a depth limit and a heuristic evaluation function to handle games that are too large to search to the end.\n\n    Implement an AdversarialGame Protocol in core.py to define the rules of these games.\n\nTasks:\n\n    [ ] Update core.py:\n\n        Define the AdversarialGame Protocol: Must include player(state), is_terminal(state), and utility(state, player).\n\n    [ ] Implement adversarial/minimax.py:\n\n        alpha_beta_search(state, game, depth_limit): The main entry point.\n\n        Recursive helpers for max_value(state, alpha, beta) and min_value(state, alpha, beta).\n\n        Logic: If β≤α, stop exploring the current branch (the pruning step).\n\n    [ ] Create the \"Tic-Tac-Toe\" Domain:\n\n        Define the board and legal moves.\n\n        Assign utility: +1 for Win, -1 for Loss, 0 for Draw.\n\n    [ ] Unit Testing:\n\n        Create tests/test_adversarial.py.\n\n        Verify the AI is \"unbeatable\" in Tic-Tac-Toe (it should never lose to a random player or itself).\n\nAcceptance Criteria:\n\n    [ ] The AI never loses a game of Tic-Tac-Toe.\n\n    [ ] A comparison of \"Nodes Visited\" shows that Minimax with Pruning visits fewer nodes than pure Minimax.\n\n    [ ] The algorithm correctly respects the depth_limit and uses the heuristic when the limit is reached.\n\nDependencies:\n\nIssue #2 (Core Architecture)","labels":[],"number":6,"state":"OPEN","title":"Issue #6 – Minimax & Alpha-Beta Pruning"},{"body":"Description: Implement a specialized solver for Constraint Satisfaction Problems in csp/solvers.py. This uses a refined version of Depth-First Search called Backtracking Search. Unlike standard search, CSP solvers assign values to variables one by one and \"prune\" (abandon) branches of the search tree as soon as a rule is broken.\n\nObjectives:\n\n    Implement the Backtracking Search algorithm.\n\n    Support Early Failure Detection: Check if the current partial assignment violates any constraints before continuing.\n\n    Implement the Minimum Remaining Values (MRV) heuristic (choosing the \"most restricted\" variable first).\n\n    Model the \"Map Coloring\" problem (e.g., coloring regions so no two neighbors share a color) as a test case.\n\nTasks:\n\n    [ ] Implement csp/solvers.py:\n\n        backtracking_search(csp): A recursive function that assigns values to variables.\n\n        Logic: If assignment is complete, return it. Select unassigned variable. For each value, if consistent, add to assignment and recurse.\n\n    [ ] Define CSP Structure:\n\n        Create a CSP class that holds: variables, domains (possible values), and constraints (the rules).\n\n    [ ] Implement MRV Heuristic:\n\n        Add a helper to pick the variable with the smallest number of \"legal\" values remaining. This significantly speeds up the solver.\n\n    [ ] Unit Testing:\n\n        Create tests/test_csp.py.\n\n        Solve the \"Australia Map Coloring\" problem or a \"4-Queens\" puzzle.\n\nAcceptance Criteria:\n\n    [ ] The solver can find a valid assignment for a 5-variable Map Coloring problem instantly.\n\n    [ ] The solver correctly identifies when a problem is \"Inconsistent\" (unsolvable).\n\n    [ ] The code maintains a clean separation between the Solver and the Problem (CSP Definition).\n\nDependencies:\n\nIssue #2 (Core Architecture)","labels":[],"number":5,"state":"CLOSED","title":"Issue #5 – Constraint Satisfaction Problems (CSP)"},{"body":"Description: Implement A Search* and Uniform Cost Search (UCS) in searching/informed.py. Unlike the previous algorithms, these use a Priority Queue to always expand the most \"promising\" node first. This issue also involves creating a 2D Grid/Maze environment to demonstrate that A* is significantly more efficient than BFS.\n\nObjectives:\n\n    Implement a Priority Queue logic using Python’s heapq module.\n\n    Implement Uniform Cost Search (UCS): Finding the cheapest path based on actual costs (g(n)).\n\n    Implement A* Search: Optimizing search using f(n)=g(n)+h(n).\n\n    Create a Manhattan Distance heuristic for grid-based problems.\n\nTasks:\n\n    [ ] Implement searching/informed.py:\n\n        astar(problem): Must use a heapq as the frontier.\n\n        Logic: Pull node with lowest f_score, check if goal, if not, expand.\n\n        Visited/Explored Set: Crucial—A* must keep track of the \"lowest cost to reach a state\" to avoid redundant work.\n\n    [ ] Implement Manhattan Distance Helper:\n\n        A function that calculates ∣x1​−x2​∣+∣y1​−y2​∣ for 2D grids.\n\n    [ ] Create the \"2D Maze\" Domain:\n\n        Define a grid with obstacles.\n\n        Implement step_cost (e.g., 1.0 for every move).\n\n    [ ] Performance Benchmarking:\n\n        Create a script that counts the number of nodes expanded by BFS (from Issue #3) vs. A* for the same maze.\n\n        Document the result in CONCEPTS.md.\n\nAcceptance Criteria:\n\n    [ ] A* successfully finds the shortest path in a complex 10x10 maze.\n\n    [ ] The algorithm visits significantly fewer nodes than BFS for the same goal.\n\n    [ ] The code correctly handles the MetricProblem mixin from core.py.\n\n    [ ] All functions are fully type-hinted and pass pytest.\n\nDependencies:\n\nIssue #2 (Core Architecture)\n\nIssue #3 (For comparison benchmarks)","labels":[],"number":4,"state":"CLOSED","title":"Issue #4 – Informed Search & Heuristics (A*)"},{"body":"Description: Implement the foundational \"blind\" search algorithms: Breadth-First Search (BFS) and Depth-First Search (DFS). These algorithms will use the Node and SearchDomain structures created in Issue #2 to navigate state spaces. We will also implement a specific \"River Crossing\" puzzle to prove the algorithms work.\n\nObjectives:\n\n    Implement BFS to find the shallowest (shortest) path to a goal.\n\n    Implement DFS to explore deep into a state space.\n\n    Handle Graph Search: Ensure the algorithms don't get stuck in infinite loops by tracking visited states.\n\n    Model the \"Wolf, Goat, and Cabbage\" puzzle as our first real-world test case.\n\nTasks:\n\n    [ ] Implement searching/uninformed.py:\n\n        bfs(problem): Use collections.deque as a FIFO (First-In, First-Out) queue.\n\n        dfs(problem): Use a standard Python list as a LIFO (Last-In, First-Out) stack.\n\n        Ensure both return a Node (the goal node) or None if no solution exists.\n\n    [ ] Implement Path Reconstruction:\n\n        Add a utility (either in core.py or uninformed.py) to trace back from a goal node to the root to return the sequence of actions.\n\n    [ ] Create the \"Wolf, Goat, Cabbage\" Domain:\n\n        Define the state (where is the boat, the wolf, etc.).\n\n        Define the rules (e.g., if the wolf and goat are alone, the goat gets eaten).\n\n    [ ] Unit Testing:\n\n        Create tests/test_uninformed.py.\n\n        Verify that BFS finds the optimal solution (minimum moves) for the puzzle.\n\nAcceptance Criteria:\n\n    [ ] BFS and DFS successfully solve the \"Wolf, Goat, Cabbage\" puzzle.\n\n    [ ] The algorithms correctly identify \"Unsolvable\" problems without crashing or looping infinitely.\n\n    [ ] Code follows the Style Guide (Type Hinting and Google Style docstrings).\n\n    [ ] A comparison is documented in CONCEPTS.md explaining when to use a Queue (BFS) vs. a Stack (DFS).\n\nDependencies:\n\nIssue #2 (Core Architecture)","labels":[],"number":3,"state":"CLOSED","title":"Issue #3 – Uninformed Search Algorithms"},{"body":"Description: Implement the backbone of Pathos AI in core.py. This involves creating the Node class—which handles the bookkeeping for search algorithms—and defining the Protocols that dictate how problems must be structured to be \"solvable\" by our library.\n\nObjectives:\n\n    Implement a memory-efficient Node class using __slots__.\n\n    Define the SearchDomain Protocol to ensure type safety across the library.\n\n    Implement the expand logic, which allows a Node to generate its successor states automatically.\n\n    Create the CONCEPTS.md entry for \"State vs. Node\" to clarify this distinction for learners.\n\nTasks:\n\n    [ ] Implement the Node Class:\n\n        Use Generic[State, Action] for strict type hinting.\n\n        Implement __slots__ to minimize memory footprint during massive searches.\n\n        Implement __lt__ (less than) to allow the Node to work natively with Python's heapq (Priority Queues).\n\n    [ ] Define Protocols (Interfaces):\n\n        SearchDomain: Must define actions(state) and result(state, action).\n\n        GoalOriented: Extends SearchDomain, adding is_goal(state).\n\n        CostSensitive: Extends SearchDomain, adding step_cost(state, action, next_state).\n\n    [ ] Develop the Node.expand Method:\n\n        Logic: Iterate through actions, generate new states, and wrap them in new Node objects.\n\n        Feature: Auto-calculation of depth and path_cost.\n\n    [ ] Unit Testing:\n\n        Create tests/test_core.py.\n\n        Define a \"Dummy Problem\" (e.g., a simple number incrementer) to verify that calling node.expand() returns the expected children.\n\nAcceptance Criteria:\n\n    [ ] pathos/core.py contains the Node class and all specified Protocols.\n\n    [ ] Node objects are comparable via < based on their f_score.\n\n    [ ] pytest tests/test_core.py passes with 100% coverage for the expansion logic.\n\n    [ ] Memory check: A simple script can initialize 1,000,000 Nodes without crashing a standard environment (verifying __slots__ effectiveness).\n\nDependencies:\n\nIssue #1 (Project Initialization)","labels":[],"number":2,"state":"CLOSED","title":"Issue #2 – Core Architecture & The Universal Node"},{"body":"Description: Configure a reproducible development environment for the Pathos AI project and establish a uniform commit convention. This ensures that the project is package-ready (PyPI) from day one and that the history remains clean using Conventional Commits.\n\nObjectives:\n\n    Ensure that any developer (or the user) can clone the repository and, following a few steps, have all dependencies (both Pure Python and Numpy/Scipy) ready to run.\n\n    Ensure commit messages follow a standard, readable format (using commitizen).\n\n    Establish the directory structure defined in the Project Specification.\n\nTasks:\n\n    [ ] Initialize Repository Structure:\n\n        Create the root folder pathos-ai/.\n\n        Create the package source pathos/ and submodules (core.py, searching/, adversarial/, optimization/, csp/) with empty __init__.py files.\n\n        Create auxiliary folders: tests/, docs/, scripts/.\n\n    [ ] Define Dependency Management:\n\n        Create a pyproject.toml file. This is the modern standard for Python packaging.\n\n        Define dependencies: numpy, scipy.\n\n        Define dev-dependencies: pytest, commitizen, mypy (for strict typing), black (for formatting).\n\n    [ ] Setup Development Documentation:\n\n        Create docs/development.md documenting how to set up the environment (e.g., python -m venv .venv).\n\n        Create CONCEPTS.md (The \"Project Bible\") for our learning notes.\n\n    [ ] Configure Commit Convention:\n\n        Initialize commitizen configuration in pyproject.toml.\n\n        Set the standard to cz_conventional (Conventional Commits).\n\n    [ ] Create Setup Script (Optional but recommended):\n\n        Create scripts/setup_dev.sh (or Makefile) to automate the installation of dependencies.\n\nAcceptance Criteria:\n\n    [ ] The directory structure matches the \"Architectural Vision\" in the spec exactly.\n\n    [ ] A pyproject.toml exists, and running pip install .[dev] (or equivalent) installs all necessary libraries.\n\n    [ ] docs/development.md exists and clearly explains how to start coding.\n\n    [ ] CONCEPTS.md is created with the initial glossary entries.\n\n    [ ] Running cz commit opens the interactive commit menu, ensuring the tool is correctly configured.\n\nDependencies:\n\n    None (This is the root issue).","labels":[],"number":1,"state":"CLOSED","title":"Issue #1 – Project Initialization & Environment Setup"}]
